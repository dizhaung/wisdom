{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Wisdom Stream Processor Wisdom is an adaptive, distributed and functionally auto-scaling stream processor written in Java 11 using modern architecture from the scratch. Though I designed and developed Wisdom for my research project, my OCD in designing scalable and extensible architecture made Wisdom an industry ready product with all basic requirements. I admit that Wisdom may lack some features that you are looking for. For example, right now Wisdom does not support database integration. It is only because my research did not require those features and I do not have enough time to add those extra nice-to-have features. Research Work Query optimization and functionally auto-scaling deployment of Wisdom Stream Processor are published in the GLOBECOM 2018 conference. You can find more details about the research in Real-time Intrusion Detection in Network Traffic Using Adaptive and Auto-scaling Stream Processor for more details. If you are using Wisdom for your research work, please cite Wisdom using the following paper: Citation: Loganathan, G., Samarabandu, J., & Wang, X. (2018). Real-time Intrusion Detection in Network Traffic Using Adaptive and Auto-scaling Stream Processor. In 2018 IEEE Global Communications Conference (GLOBECOM) (GLOBECOM 2018). Abu Dhabi, UAE. BibTex @INPROCEEDINGS{Gobinath:Wisdom, AUTHOR=\"Gobinath Loganathan and Jagath Samarabandu and Xianbin Wang\", TITLE=\"Real-time Intrusion Detection in Network Traffic Using Adaptive and Auto-scaling Stream Processor\", BOOKTITLE=\"2018 IEEE Canadian Conference on Electrical \\& Computer Engineering (CCECE) (CCECE 2018)\", ADDRESS=\"Abu Dhabi, UAE\", DAYS=13, MONTH=dec, YEAR=2018, ABSTRACT=\"Advanced intrusion detection systems are beginning to utilize the power and flexibility offered by Complex Event Processing (CEP) engines. Adapting to new attacks and optimizing CEP rules are two challenges in this domain. Optimizing CEP rules requires a complete framework which can be ported to stream processors because a CEP rule cannot run without a stream processor. External dependencies of stream processors make CEP rule a black box which is hard to optimize. In this paper, we present a novel adaptive and functionally auto-scaling stream processor: \"Wisdom\" with a built-in hybrid optimizer developed using Particle Swarm Optimization, and Bisection algorithms to optimize CEP rule parameters. We show that an adaptive ``Wisdom'' rule tuned by the proposed optimization algorithm is able to detect selected attacks in CICIDS 2017 dataset with an average precision of 99.98\\% and an average recall of 93.42\\% while processing over 2.5 million events per second. The proposed distributed functionally auto-scaling deployment mode consumes significantly fewer system resources than the monolithic deployment of CEP rules.\" } If you like to invest in Wisdom, please contact me via slgobinath@gmail.com . If you are looking for an easy to use stream processor with fresh design and less complexity for your research, you are at the right place. Just drop me an email: slgobinath@gmail.com .","title":"Home"},{"location":"#wisdom-stream-processor","text":"Wisdom is an adaptive, distributed and functionally auto-scaling stream processor written in Java 11 using modern architecture from the scratch. Though I designed and developed Wisdom for my research project, my OCD in designing scalable and extensible architecture made Wisdom an industry ready product with all basic requirements. I admit that Wisdom may lack some features that you are looking for. For example, right now Wisdom does not support database integration. It is only because my research did not require those features and I do not have enough time to add those extra nice-to-have features.","title":"Wisdom Stream Processor"},{"location":"#research-work","text":"Query optimization and functionally auto-scaling deployment of Wisdom Stream Processor are published in the GLOBECOM 2018 conference. You can find more details about the research in Real-time Intrusion Detection in Network Traffic Using Adaptive and Auto-scaling Stream Processor for more details. If you are using Wisdom for your research work, please cite Wisdom using the following paper: Citation: Loganathan, G., Samarabandu, J., & Wang, X. (2018). Real-time Intrusion Detection in Network Traffic Using Adaptive and Auto-scaling Stream Processor. In 2018 IEEE Global Communications Conference (GLOBECOM) (GLOBECOM 2018). Abu Dhabi, UAE. BibTex @INPROCEEDINGS{Gobinath:Wisdom, AUTHOR=\"Gobinath Loganathan and Jagath Samarabandu and Xianbin Wang\", TITLE=\"Real-time Intrusion Detection in Network Traffic Using Adaptive and Auto-scaling Stream Processor\", BOOKTITLE=\"2018 IEEE Canadian Conference on Electrical \\& Computer Engineering (CCECE) (CCECE 2018)\", ADDRESS=\"Abu Dhabi, UAE\", DAYS=13, MONTH=dec, YEAR=2018, ABSTRACT=\"Advanced intrusion detection systems are beginning to utilize the power and flexibility offered by Complex Event Processing (CEP) engines. Adapting to new attacks and optimizing CEP rules are two challenges in this domain. Optimizing CEP rules requires a complete framework which can be ported to stream processors because a CEP rule cannot run without a stream processor. External dependencies of stream processors make CEP rule a black box which is hard to optimize. In this paper, we present a novel adaptive and functionally auto-scaling stream processor: \"Wisdom\" with a built-in hybrid optimizer developed using Particle Swarm Optimization, and Bisection algorithms to optimize CEP rule parameters. We show that an adaptive ``Wisdom'' rule tuned by the proposed optimization algorithm is able to detect selected attacks in CICIDS 2017 dataset with an average precision of 99.98\\% and an average recall of 93.42\\% while processing over 2.5 million events per second. The proposed distributed functionally auto-scaling deployment mode consumes significantly fewer system resources than the monolithic deployment of CEP rules.\" } If you like to invest in Wisdom, please contact me via slgobinath@gmail.com . If you are looking for an easy to use stream processor with fresh design and less complexity for your research, you are at the right place. Just drop me an email: slgobinath@gmail.com .","title":"Research Work"},{"location":"deploy-wisdom/","text":"Wisdom offers three different deployment options: (1) in-app usage as a Java library, (2) stand-alone deployment as a microservice, (3) Wisdom Orchestra deployment. Wisdom Library This is the recommended method to use Wisdom if you are developing a new Wisdom rule or if you want to playwith Wisdom. It is also recommended for applications require in-app complex event processing. Please check the Getting Started guidelines to use Wisdom as a library. Wisdom Service Wisdom Service is recommended if you are deploying a stand-alone CEP rule which requires HTTP endpoints and/or more system resources to be allocated. You can either use the Wisdom server to run your query or develop your own microservice to run your Wisdom app. Deploy Wisdom Query Using Wisdom Server Step 1: Email the author( slgobinath@gmail.com ) and get the Wisdom Server pack. Step 2: Extract the zip file and navigate into the extracted directory. unzip product-wisdom-0.0.1.zip cd product-wisdom-0.0.1 Step 3: Save the following Wisdom query into the artifacts directory with a name: stock_filter.wisdomql . @app(name='stock_filter', version='1.0.0') @source(type='http', mapping='json') def stream StockStream; @sink(type='console') def stream OutputStream; @query(name='FilterQuery') from StockStream filter symbol == 'AMAZON' select symbol, price insert into OutputStream; Step 4: Start the Wisdom Service on port 8080 using the following command: sh wisdom-service.sh --port 8080 artifacts/stock_filter.wisdomql Step 5: Using Postman or similar tools, send an event using HTTP POST request. For simplicity, we use curl to send the request. curl -d '{\"symbol\": \"AMAZON\", \"price\": 120.0, \"volume\": 10}' -H \"Content-Type: application/json\" -X POST http://localhost:8080/WisdomApp/StockStream After sending this request, you should see the the following output in the terminal running Wisdom service: Event{timestamp=1524757628355, stream=OutputStream, data={symbol=AMAZON, price=120.0}, expired=false} Wisdom Orchestra Wisdom Orchestra deployment is a fancy name I use to refer managing Wisdom instances using Wisdom Manager. Wisdom Manager is a specially designed tool to deploy and manage Wisdom services. It can be used to deploy stand-alone Wisdom services or to deploy self-boosting Wisdom environment. Wisdom Manager often requires Apache Kafka to coordinate and communicate with Wisdom instances. Therefore, please setup and start Apache Kakfa before running Wisdom Manager. Step 1: Download and extract the latest Apache Kafka anywhere in your system. Step 2: Start Apache Kafka using the following two commands from KAFKA_HOME . # Start Zookeeper server sh bin/zookeeper-server-start.sh config/zookeeper.properties # Start Kafka server sh bin/kafka-server-start.sh config/server.properties Step 3: Open another terminal in WISDOM_HOME and start the Wisdom Manager. sh wisdom-manager.sh Step 4: Send an HTTP POST request with a Wisdom query along with a port to start that query. curl -d \"{\\\"query\\\": \\\"@app(name='stock_filter', version='1.0.0') \\ @source(type='http', mapping='json') \\ def stream StockStream; \\ @sink(type='file.text', path='/tmp/OutputStream.txt') \\ def stream OutputStream; \\ @query(name='FilterQuery') \\ from StockStream \\ filter symbol == 'AMAZON' \\ select symbol, price \\ insert into OutputStream;\\\", \\\"port\\\": 8085}\" -H \"Content-Type: application/json\" -X POST http://localhost:8080/WisdomManager/app Note that the OutputStream sink is a text file: /tmp/OutputStream.txt . Step 5: Start stock_filter by sending another POST request. curl -X POST http://localhost:8080/WisdomManager/start/stock_filter Step 06: Test stock_filter by sending a stock event. curl -d '{\"symbol\": \"AMAZON\", \"price\": 120.0, \"volume\": 10}' -H \"Content-Type: application/json\" -X POST http://localhost:8085/WisdomApp/StockStream After sending above event, you should have a file /tmp/OutputStream.txt with the following content: Event{timestamp=1524761284277, stream=OutputStream, data={symbol=AMAZON, price=120.0}, expired=false} Step 07: Stop the stock_filter app curl -X POST http://localhost:8080/WisdomManager/stop/stock_filter Step 08: Delete the stock_filter app curl -X DELETE http://localhost:8080/WisdomManager/app/stock_filter Step 09: Stop the Wisdom Manager curl -X POST http://localhost:8080/WisdomManager/stop","title":"Deploy Wisdom"},{"location":"deploy-wisdom/#wisdom-library","text":"This is the recommended method to use Wisdom if you are developing a new Wisdom rule or if you want to playwith Wisdom. It is also recommended for applications require in-app complex event processing. Please check the Getting Started guidelines to use Wisdom as a library.","title":"Wisdom Library"},{"location":"deploy-wisdom/#wisdom-service","text":"Wisdom Service is recommended if you are deploying a stand-alone CEP rule which requires HTTP endpoints and/or more system resources to be allocated. You can either use the Wisdom server to run your query or develop your own microservice to run your Wisdom app. Deploy Wisdom Query Using Wisdom Server Step 1: Email the author( slgobinath@gmail.com ) and get the Wisdom Server pack. Step 2: Extract the zip file and navigate into the extracted directory. unzip product-wisdom-0.0.1.zip cd product-wisdom-0.0.1 Step 3: Save the following Wisdom query into the artifacts directory with a name: stock_filter.wisdomql . @app(name='stock_filter', version='1.0.0') @source(type='http', mapping='json') def stream StockStream; @sink(type='console') def stream OutputStream; @query(name='FilterQuery') from StockStream filter symbol == 'AMAZON' select symbol, price insert into OutputStream; Step 4: Start the Wisdom Service on port 8080 using the following command: sh wisdom-service.sh --port 8080 artifacts/stock_filter.wisdomql Step 5: Using Postman or similar tools, send an event using HTTP POST request. For simplicity, we use curl to send the request. curl -d '{\"symbol\": \"AMAZON\", \"price\": 120.0, \"volume\": 10}' -H \"Content-Type: application/json\" -X POST http://localhost:8080/WisdomApp/StockStream After sending this request, you should see the the following output in the terminal running Wisdom service: Event{timestamp=1524757628355, stream=OutputStream, data={symbol=AMAZON, price=120.0}, expired=false}","title":"Wisdom Service"},{"location":"deploy-wisdom/#wisdom-orchestra","text":"Wisdom Orchestra deployment is a fancy name I use to refer managing Wisdom instances using Wisdom Manager. Wisdom Manager is a specially designed tool to deploy and manage Wisdom services. It can be used to deploy stand-alone Wisdom services or to deploy self-boosting Wisdom environment. Wisdom Manager often requires Apache Kafka to coordinate and communicate with Wisdom instances. Therefore, please setup and start Apache Kakfa before running Wisdom Manager. Step 1: Download and extract the latest Apache Kafka anywhere in your system. Step 2: Start Apache Kafka using the following two commands from KAFKA_HOME . # Start Zookeeper server sh bin/zookeeper-server-start.sh config/zookeeper.properties # Start Kafka server sh bin/kafka-server-start.sh config/server.properties Step 3: Open another terminal in WISDOM_HOME and start the Wisdom Manager. sh wisdom-manager.sh Step 4: Send an HTTP POST request with a Wisdom query along with a port to start that query. curl -d \"{\\\"query\\\": \\\"@app(name='stock_filter', version='1.0.0') \\ @source(type='http', mapping='json') \\ def stream StockStream; \\ @sink(type='file.text', path='/tmp/OutputStream.txt') \\ def stream OutputStream; \\ @query(name='FilterQuery') \\ from StockStream \\ filter symbol == 'AMAZON' \\ select symbol, price \\ insert into OutputStream;\\\", \\\"port\\\": 8085}\" -H \"Content-Type: application/json\" -X POST http://localhost:8080/WisdomManager/app Note that the OutputStream sink is a text file: /tmp/OutputStream.txt . Step 5: Start stock_filter by sending another POST request. curl -X POST http://localhost:8080/WisdomManager/start/stock_filter Step 06: Test stock_filter by sending a stock event. curl -d '{\"symbol\": \"AMAZON\", \"price\": 120.0, \"volume\": 10}' -H \"Content-Type: application/json\" -X POST http://localhost:8085/WisdomApp/StockStream After sending above event, you should have a file /tmp/OutputStream.txt with the following content: Event{timestamp=1524761284277, stream=OutputStream, data={symbol=AMAZON, price=120.0}, expired=false} Step 07: Stop the stock_filter app curl -X POST http://localhost:8080/WisdomManager/stop/stock_filter Step 08: Delete the stock_filter app curl -X DELETE http://localhost:8080/WisdomManager/app/stock_filter Step 09: Stop the Wisdom Manager curl -X POST http://localhost:8080/WisdomManager/stop","title":"Wisdom Orchestra"},{"location":"getting-started/","text":"Wisdom offers complete Java API and Wisdom Query Language to develop Complex Event Processing (CEP) applications. Wisdom can be used as a Java library or standalone service. Java library is recommended for testing purposes, small-scale applications and Android applications. If you are developing a resource consuming CEP application, it is recommended to use Wisdom Service. Wisdom Service is the only way to use HTTP Source and Sinks. This section explains how to create a simple CEP application using Wisdom Java API and Wisdom Query. Requirements Make sure that you have set up the following softwares in your system before building Wisdom. Java 11 (or latest) Apache Maven Apache Kafka (for functionally auto-scaling deployment) Installation Clone Wisdom source code git clone https://github.com/slgobinath/wisdom.git Open your terminal and change directory cd wisdom Compile and install Wisdom using Apache Maven mvn clean install How to use? Wisdom Java API Create a new Maven Project in your favorite IDE. We use IntelliJ IDEA throughout this document. Open the pom.xml file add wisdom-core and optionally logback dependencies as shown below: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>com.javahelps</groupId> <artifactId>wisdom-java-api</artifactId> <version>1.0-SNAPSHOT</version> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <source>11</source> <target>11</target> </configuration> </plugin> </plugins> </build> <properties> <wisdom.version>0.0.1</wisdom.version> <logback.version>1.2.3</logback.version> </properties> <dependencies> <dependency> <groupId>com.javahelps.wisdom</groupId> <artifactId>wisdom-core</artifactId> <version>${wisdom.version}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-core</artifactId> <version>${logback.version}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version>${logback.version}</version> </dependency> </dependencies> </project> Create a new Java class com.javahelps.helloworld.javaapi.HelloWorld with the following code. package com.javahelps.helloworld.javaapi; import com.javahelps.wisdom.core.WisdomApp; import com.javahelps.wisdom.core.operator.Operator; import com.javahelps.wisdom.core.stream.InputHandler; import com.javahelps.wisdom.core.util.EventGenerator; import com.javahelps.wisdom.core.util.EventPrinter; public class HelloWorld { public static void main(String[] args) { // Create a Wisdom application WisdomApp app = new WisdomApp(\"WisdomApp\", \"1.0.0\"); // Define streams app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); // Create a query app.defineQuery(\"FilterQuery\") .from(\"StockStream\") .filter(Operator.EQUALS(\"symbol\", \"AMAZON\")) .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); // Add output stream callback app.addCallback(\"OutputStream\", EventPrinter::print); // Get an input handler InputHandler inputHandler = app.getInputHandler(\"StockStream\"); // Start the application app.start(); // Send three inputs inputHandler.send(EventGenerator.generate(\"symbol\", \"GOOGLE\", \"price\", 10.5, \"volume\", 10L)); inputHandler.send(EventGenerator.generate(\"symbol\", \"AMAZON\", \"price\", 20.5, \"volume\", 20L)); inputHandler.send(EventGenerator.generate(\"symbol\", \"FACEBOOK\", \"price\", 30.5, \"volume\", 30L)); // Shutdown the application app.shutdown(); } } Above code creates Wisdom application with two streams: StockStream and OutputStream , and a query named FilterQuery . The FilterQuery filters stock events of AMAZON , select symbol and price , and insert them into the OutputStream . InputHandler is used to feed events to a stream and callback is used to receive events from a stream. Running this code should print an output similar to this: [Event{timestamp=1524709449322, stream=OutputStream, data={symbol=AMAZON, price=20.5}, expired=false}] As you can see, above Wisdom app filters events having symbol equal to AMAZON and prints them to the console. Wisdom Query Above Wisdom application can be defined using the folloing Wisdom query: @app(name='WisdomApp', version='1.0.0') def stream StockStream; def stream OutputStream; @query(name='FilterQuery') from StockStream filter symbol == 'AMAZON' select symbol, price insert into OutputStream; To use this query in a Java application, create a new Maven project in your favorite IDE. Open the pom.xml file and add wisdom-core , wisdom-query and optionally logback dependencies as shown below: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>com.javahelps</groupId> <artifactId>wisdom-java-api</artifactId> <version>1.0-SNAPSHOT</version> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <source>11</source> <target>11</target> </configuration> </plugin> </plugins> </build> <properties> <wisdom.version>0.0.1</wisdom.version> <logback.version>1.2.3</logback.version> </properties> <dependencies> <dependency> <groupId>com.javahelps.wisdom</groupId> <artifactId>wisdom-core</artifactId> <version>${wisdom.version}</version> </dependency> <dependency> <groupId>com.javahelps.wisdom</groupId> <artifactId>wisdom-query</artifactId> <version>${wisdom.version}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-core</artifactId> <version>${logback.version}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version>${logback.version}</version> </dependency> </dependencies> </project> Create a new Java class com.javahelps.helloworld.wisdomql.HelloWorld with the following code. package com.javahelps.helloworld.wisdomql; import com.javahelps.wisdom.core.WisdomApp; import com.javahelps.wisdom.core.stream.InputHandler; import com.javahelps.wisdom.core.util.EventGenerator; import com.javahelps.wisdom.core.util.EventPrinter; import com.javahelps.wisdom.query.WisdomCompiler; public class HelloWorld { public static void main(String[] args) { String query = \"@app(name='WisdomApp', version='1.0.0') \" + \"def stream StockStream; \" + \"def stream OutputStream; \" + \" \" + \"@query(name='FilterQuery') \" + \"from StockStream \" + \"filter symbol == 'AMAZON' \" + \"select symbol, price \" + \"insert into OutputStream;\"; // Create a Wisdom application WisdomApp app = WisdomCompiler.parse(query); // Add output stream callback app.addCallback(\"OutputStream\", EventPrinter::print); // Get an input handler InputHandler inputHandler = app.getInputHandler(\"StockStream\"); // Start the application app.start(); // Send three inputs inputHandler.send(EventGenerator.generate(\"symbol\", \"GOOGLE\", \"price\", 10.5, \"volume\", 10L)); inputHandler.send(EventGenerator.generate(\"symbol\", \"AMAZON\", \"price\", 20.5, \"volume\", 20L)); inputHandler.send(EventGenerator.generate(\"symbol\", \"FACEBOOK\", \"price\", 30.5, \"volume\", 30L)); // Shutdown the application app.shutdown(); } } Above code replaces the Java API used in previous example by the Wisdom query to construct a Wisdom app. Once the Wisdom app is created, creating InputHandler and sending events are same as the previous example.","title":"Getting Started"},{"location":"getting-started/#requirements","text":"Make sure that you have set up the following softwares in your system before building Wisdom. Java 11 (or latest) Apache Maven Apache Kafka (for functionally auto-scaling deployment)","title":"Requirements"},{"location":"getting-started/#installation","text":"Clone Wisdom source code git clone https://github.com/slgobinath/wisdom.git Open your terminal and change directory cd wisdom Compile and install Wisdom using Apache Maven mvn clean install","title":"Installation"},{"location":"getting-started/#how-to-use","text":"","title":"How to use?"},{"location":"getting-started/#wisdom-java-api","text":"Create a new Maven Project in your favorite IDE. We use IntelliJ IDEA throughout this document. Open the pom.xml file add wisdom-core and optionally logback dependencies as shown below: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>com.javahelps</groupId> <artifactId>wisdom-java-api</artifactId> <version>1.0-SNAPSHOT</version> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <source>11</source> <target>11</target> </configuration> </plugin> </plugins> </build> <properties> <wisdom.version>0.0.1</wisdom.version> <logback.version>1.2.3</logback.version> </properties> <dependencies> <dependency> <groupId>com.javahelps.wisdom</groupId> <artifactId>wisdom-core</artifactId> <version>${wisdom.version}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-core</artifactId> <version>${logback.version}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version>${logback.version}</version> </dependency> </dependencies> </project> Create a new Java class com.javahelps.helloworld.javaapi.HelloWorld with the following code. package com.javahelps.helloworld.javaapi; import com.javahelps.wisdom.core.WisdomApp; import com.javahelps.wisdom.core.operator.Operator; import com.javahelps.wisdom.core.stream.InputHandler; import com.javahelps.wisdom.core.util.EventGenerator; import com.javahelps.wisdom.core.util.EventPrinter; public class HelloWorld { public static void main(String[] args) { // Create a Wisdom application WisdomApp app = new WisdomApp(\"WisdomApp\", \"1.0.0\"); // Define streams app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); // Create a query app.defineQuery(\"FilterQuery\") .from(\"StockStream\") .filter(Operator.EQUALS(\"symbol\", \"AMAZON\")) .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); // Add output stream callback app.addCallback(\"OutputStream\", EventPrinter::print); // Get an input handler InputHandler inputHandler = app.getInputHandler(\"StockStream\"); // Start the application app.start(); // Send three inputs inputHandler.send(EventGenerator.generate(\"symbol\", \"GOOGLE\", \"price\", 10.5, \"volume\", 10L)); inputHandler.send(EventGenerator.generate(\"symbol\", \"AMAZON\", \"price\", 20.5, \"volume\", 20L)); inputHandler.send(EventGenerator.generate(\"symbol\", \"FACEBOOK\", \"price\", 30.5, \"volume\", 30L)); // Shutdown the application app.shutdown(); } } Above code creates Wisdom application with two streams: StockStream and OutputStream , and a query named FilterQuery . The FilterQuery filters stock events of AMAZON , select symbol and price , and insert them into the OutputStream . InputHandler is used to feed events to a stream and callback is used to receive events from a stream. Running this code should print an output similar to this: [Event{timestamp=1524709449322, stream=OutputStream, data={symbol=AMAZON, price=20.5}, expired=false}] As you can see, above Wisdom app filters events having symbol equal to AMAZON and prints them to the console.","title":"Wisdom Java API"},{"location":"getting-started/#wisdom-query","text":"Above Wisdom application can be defined using the folloing Wisdom query: @app(name='WisdomApp', version='1.0.0') def stream StockStream; def stream OutputStream; @query(name='FilterQuery') from StockStream filter symbol == 'AMAZON' select symbol, price insert into OutputStream; To use this query in a Java application, create a new Maven project in your favorite IDE. Open the pom.xml file and add wisdom-core , wisdom-query and optionally logback dependencies as shown below: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>com.javahelps</groupId> <artifactId>wisdom-java-api</artifactId> <version>1.0-SNAPSHOT</version> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <source>11</source> <target>11</target> </configuration> </plugin> </plugins> </build> <properties> <wisdom.version>0.0.1</wisdom.version> <logback.version>1.2.3</logback.version> </properties> <dependencies> <dependency> <groupId>com.javahelps.wisdom</groupId> <artifactId>wisdom-core</artifactId> <version>${wisdom.version}</version> </dependency> <dependency> <groupId>com.javahelps.wisdom</groupId> <artifactId>wisdom-query</artifactId> <version>${wisdom.version}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-core</artifactId> <version>${logback.version}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version>${logback.version}</version> </dependency> </dependencies> </project> Create a new Java class com.javahelps.helloworld.wisdomql.HelloWorld with the following code. package com.javahelps.helloworld.wisdomql; import com.javahelps.wisdom.core.WisdomApp; import com.javahelps.wisdom.core.stream.InputHandler; import com.javahelps.wisdom.core.util.EventGenerator; import com.javahelps.wisdom.core.util.EventPrinter; import com.javahelps.wisdom.query.WisdomCompiler; public class HelloWorld { public static void main(String[] args) { String query = \"@app(name='WisdomApp', version='1.0.0') \" + \"def stream StockStream; \" + \"def stream OutputStream; \" + \" \" + \"@query(name='FilterQuery') \" + \"from StockStream \" + \"filter symbol == 'AMAZON' \" + \"select symbol, price \" + \"insert into OutputStream;\"; // Create a Wisdom application WisdomApp app = WisdomCompiler.parse(query); // Add output stream callback app.addCallback(\"OutputStream\", EventPrinter::print); // Get an input handler InputHandler inputHandler = app.getInputHandler(\"StockStream\"); // Start the application app.start(); // Send three inputs inputHandler.send(EventGenerator.generate(\"symbol\", \"GOOGLE\", \"price\", 10.5, \"volume\", 10L)); inputHandler.send(EventGenerator.generate(\"symbol\", \"AMAZON\", \"price\", 20.5, \"volume\", 20L)); inputHandler.send(EventGenerator.generate(\"symbol\", \"FACEBOOK\", \"price\", 30.5, \"volume\", 30L)); // Shutdown the application app.shutdown(); } } Above code replaces the Java API used in previous example by the Wisdom query to construct a Wisdom app. Once the Wisdom app is created, creating InputHandler and sending events are same as the previous example.","title":"Wisdom Query"},{"location":"why-wisdom/","text":"There are plenty of stream processors out there but Wisdom does not just increase the count. It is adaptive, distributable and self-boosting without compromising the performance. Key Features: Self-tuning queries Functionally auto-scaling deployment High throughput Lightweight Distributed Open Source Adaptive and Tunable One of the key selling point of Wisdom is its ability to tune queries for best results. Wisdom queries can be defined using variables and Wisdom will tune them based on a loss function you define. For example, we defined three Wisdom rules to detect intrusions but in none of them we defined the time window interval or the minimum count threshold. Instead, Wisdom mined and optimized those values from training data. Obtained optimal values gave us 99% accuracy in intrusion detection. Isn't that cool? For more details about the optimization algorithm, please read Real-time Intrusion Detection in Network Traffic Using Adaptive and Auto-scaling Stream Processor Performance A common limitation I have observed in dynamic stream processors is their performance bottleneck. I designed the underlying architecture of Wisdom in such a way that it is comparable with existing commercial stream processors. I developed a simple filter query to compare Wisdom with Apache Flink , WSO2 Siddhi and Esper CEP . The throughput and latency of Wisdom is closed to WSO2 Siddhi and better than Esper . Stream Processor Throughput Latency Apache Flink 6,711,544 events/sec 100 nanoseconds WSO2 Siddhi 3,811,876 events/sec 216 nanoseconds Wisdom 2,543,299 events/sec 332 nanoseconds Esper 2,247,807 events/sec 334 nanoseconds Functionally Auto-scaling In our research, we have shown that Wisdom consumes significantly fewer system resources than other distributed stream processors. The complete deployment setup and how it works are described in our research paper. Wisdom Query Wisdom Query is an SQL like query inspired by Siddhi Query . An expressive query language hides the complexity of stream processing and make it super easy to use stream processors. In addition, it lets you deploy CEP applications via REST API calls instead of transferring a deployable jar file or any other binary file.","title":"Why Wisdom?"},{"location":"why-wisdom/#adaptive-and-tunable","text":"One of the key selling point of Wisdom is its ability to tune queries for best results. Wisdom queries can be defined using variables and Wisdom will tune them based on a loss function you define. For example, we defined three Wisdom rules to detect intrusions but in none of them we defined the time window interval or the minimum count threshold. Instead, Wisdom mined and optimized those values from training data. Obtained optimal values gave us 99% accuracy in intrusion detection. Isn't that cool? For more details about the optimization algorithm, please read Real-time Intrusion Detection in Network Traffic Using Adaptive and Auto-scaling Stream Processor","title":"Adaptive and Tunable"},{"location":"why-wisdom/#performance","text":"A common limitation I have observed in dynamic stream processors is their performance bottleneck. I designed the underlying architecture of Wisdom in such a way that it is comparable with existing commercial stream processors. I developed a simple filter query to compare Wisdom with Apache Flink , WSO2 Siddhi and Esper CEP . The throughput and latency of Wisdom is closed to WSO2 Siddhi and better than Esper . Stream Processor Throughput Latency Apache Flink 6,711,544 events/sec 100 nanoseconds WSO2 Siddhi 3,811,876 events/sec 216 nanoseconds Wisdom 2,543,299 events/sec 332 nanoseconds Esper 2,247,807 events/sec 334 nanoseconds","title":"Performance"},{"location":"why-wisdom/#functionally-auto-scaling","text":"In our research, we have shown that Wisdom consumes significantly fewer system resources than other distributed stream processors. The complete deployment setup and how it works are described in our research paper.","title":"Functionally Auto-scaling"},{"location":"why-wisdom/#wisdom-query","text":"Wisdom Query is an SQL like query inspired by Siddhi Query . An expressive query language hides the complexity of stream processing and make it super easy to use stream processors. In addition, it lets you deploy CEP applications via REST API calls instead of transferring a deployable jar file or any other binary file.","title":"Wisdom Query"},{"location":"wisdom-extensions/","text":"Writing extensions for Wisdom is super easy if you know Java. An extension can be a Window , Source , Sink , or Mapper . In this section, I explain how to create a new sink to write events to a text file. All existing windows, sources, sinks, and mappers are written as extensions following the same technique. Create New Sink Extension Step 1: Create a new Maven project in your favorite IDE. Step 2: Open pom.xml file and add wisdom-core dependency as show below: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>com.javahelps</groupId> <artifactId>wisdom-java-api</artifactId> <version>1.0-SNAPSHOT</version> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <source>11</source> <target>11</target> </configuration> </plugin> </plugins> </build> <properties> <wisdom.version>0.0.1</wisdom.version> </properties> <dependencies> <dependency> <groupId>com.javahelps.wisdom</groupId> <artifactId>wisdom-core</artifactId> <version>${wisdom.version}</version> </dependency> </dependencies> </project> Step 3: Create a new class com.javahelps.wisdom.extensions.file.sink.TextFileSink and extend com.javahelps.wisdom.core.stream.output.Sink . Depending on your extension type, you may need to override different classes: Window - com.javahelps.wisdom.core.window.Window Source - com.javahelps.wisdom.core.stream.input.Source Sink - com.javahelps.wisdom.core.stream.output.Sink Mapper - com.javahelps.wisdom.core.map.Mapper package com.javahelps.wisdom.extensions.file.sink; import com.javahelps.wisdom.core.stream.output.Sink; public class TextFileSink extends Sink { } Step 4: Annotate the class using WisdomExtension annotation and define namespace as file.text which will be used to identify this sink later in Wisdom query. This step is common for all Wisdom extensions. package com.javahelps.wisdom.extensions.file.sink; import com.javahelps.wisdom.core.extension.WisdomExtension; import com.javahelps.wisdom.core.stream.output.Sink; @WisdomExtension(\"file.text\") public class TextFileSink extends Sink { } Step 5: Override all required methods. For text file sink, overriding publish method is enough. You may need start , init and stop if your sink is complex as Kafka sink. package com.javahelps.wisdom.extensions.file.sink; import com.javahelps.wisdom.core.WisdomApp; import com.javahelps.wisdom.core.event.Event; import com.javahelps.wisdom.core.exception.WisdomAppValidationException; import com.javahelps.wisdom.core.extension.WisdomExtension; import com.javahelps.wisdom.core.stream.output.Sink; import java.io.FileWriter; import java.io.IOException; import java.io.PrintWriter; import java.util.List; import java.util.Map; @WisdomExtension(\"file.text\") public class TextFileSink extends Sink { private final String path; public TextFileSink(Map<String, ?> properties) { super(properties); this.path = (String) properties.get(\"path\"); if (this.path == null) { throw new WisdomAppValidationException(\"Required property 'path' for TextFile sink not found\"); } } @Override public void start() { // Do nothing } @Override public void init(WisdomApp wisdomApp, String streamId) { } @Override public void publish(List<Event> events) throws IOException { try (PrintWriter writer = new PrintWriter(new FileWriter(this.path, true))) { for (Event event : events) { writer.println(event); } } } @Override public void stop() { // Do nothing } } Now you are ready to use this sink in your Wisdom app. Use Extension in Java API Step 1: Create a new test class in the same project com.javahelps.wisdom.extensions.file.sink.TestTextFileSink . Step 2: Create a static initialization block and import the custom extension. package com.javahelps.wisdom.extensions.file.sink; import com.javahelps.wisdom.core.extension.ImportsManager; public class TestTextFileSink { static { ImportsManager.INSTANCE.use(TextFileSink.class); } } We use ImportsManager to import selected extension, instead of searching the complete classpath to avoid unnecessary delays. It also reduces unnecessary complexities in Android applications. Step 3: Create a new Wisdom app using the file.text sink. Note that we are using the namespace file.text to create this sink. WisdomApp wisdomApp = new WisdomApp(); wisdomApp.defineStream(\"StockStream\"); wisdomApp.defineStream(\"OutputStream\"); wisdomApp.defineQuery(\"query1\") .from(\"StockStream\") .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); wisdomApp.addSink(\"OutputStream\", Sink.create(\"file.text\", Map.of(\"path\", \"output.log\"))); wisdomApp.start(); InputHandler stockStreamInputHandler = wisdomApp.getInputHandler(\"StockStream\"); stockStreamInputHandler.send(EventGenerator.generate(\"symbol\", \"IBM\", \"price\", 50.0, \"volume\", 10)); stockStreamInputHandler.send(EventGenerator.generate(\"symbol\", \"WSO2\", \"price\", 60.0, \"volume\", 15)); Use Extension in Wisdom Query Above sink can be used in a Wisdom query as given below: def stream StockStream; @sink(type='file.text', path='output.log') def stream OutputStream; from StockStream select symbol, price insert into OutputStream;JAR Deploy in Wisdom Server Step 1: Build the jar file containing com.javahelps.wisdom.extensions.file.sink.TextFileSink . mvn clean package Step 2: Copy and paste the target/xxx.jar file into WISDOM_HOME/lib directory. Step 3: Restart running Wisdom services.","title":"Writing Extensions"},{"location":"wisdom-extensions/#create-new-sink-extension","text":"Step 1: Create a new Maven project in your favorite IDE. Step 2: Open pom.xml file and add wisdom-core dependency as show below: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>com.javahelps</groupId> <artifactId>wisdom-java-api</artifactId> <version>1.0-SNAPSHOT</version> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <source>11</source> <target>11</target> </configuration> </plugin> </plugins> </build> <properties> <wisdom.version>0.0.1</wisdom.version> </properties> <dependencies> <dependency> <groupId>com.javahelps.wisdom</groupId> <artifactId>wisdom-core</artifactId> <version>${wisdom.version}</version> </dependency> </dependencies> </project> Step 3: Create a new class com.javahelps.wisdom.extensions.file.sink.TextFileSink and extend com.javahelps.wisdom.core.stream.output.Sink . Depending on your extension type, you may need to override different classes: Window - com.javahelps.wisdom.core.window.Window Source - com.javahelps.wisdom.core.stream.input.Source Sink - com.javahelps.wisdom.core.stream.output.Sink Mapper - com.javahelps.wisdom.core.map.Mapper package com.javahelps.wisdom.extensions.file.sink; import com.javahelps.wisdom.core.stream.output.Sink; public class TextFileSink extends Sink { } Step 4: Annotate the class using WisdomExtension annotation and define namespace as file.text which will be used to identify this sink later in Wisdom query. This step is common for all Wisdom extensions. package com.javahelps.wisdom.extensions.file.sink; import com.javahelps.wisdom.core.extension.WisdomExtension; import com.javahelps.wisdom.core.stream.output.Sink; @WisdomExtension(\"file.text\") public class TextFileSink extends Sink { } Step 5: Override all required methods. For text file sink, overriding publish method is enough. You may need start , init and stop if your sink is complex as Kafka sink. package com.javahelps.wisdom.extensions.file.sink; import com.javahelps.wisdom.core.WisdomApp; import com.javahelps.wisdom.core.event.Event; import com.javahelps.wisdom.core.exception.WisdomAppValidationException; import com.javahelps.wisdom.core.extension.WisdomExtension; import com.javahelps.wisdom.core.stream.output.Sink; import java.io.FileWriter; import java.io.IOException; import java.io.PrintWriter; import java.util.List; import java.util.Map; @WisdomExtension(\"file.text\") public class TextFileSink extends Sink { private final String path; public TextFileSink(Map<String, ?> properties) { super(properties); this.path = (String) properties.get(\"path\"); if (this.path == null) { throw new WisdomAppValidationException(\"Required property 'path' for TextFile sink not found\"); } } @Override public void start() { // Do nothing } @Override public void init(WisdomApp wisdomApp, String streamId) { } @Override public void publish(List<Event> events) throws IOException { try (PrintWriter writer = new PrintWriter(new FileWriter(this.path, true))) { for (Event event : events) { writer.println(event); } } } @Override public void stop() { // Do nothing } } Now you are ready to use this sink in your Wisdom app.","title":"Create New Sink Extension"},{"location":"wisdom-extensions/#use-extension-in-java-api","text":"Step 1: Create a new test class in the same project com.javahelps.wisdom.extensions.file.sink.TestTextFileSink . Step 2: Create a static initialization block and import the custom extension. package com.javahelps.wisdom.extensions.file.sink; import com.javahelps.wisdom.core.extension.ImportsManager; public class TestTextFileSink { static { ImportsManager.INSTANCE.use(TextFileSink.class); } } We use ImportsManager to import selected extension, instead of searching the complete classpath to avoid unnecessary delays. It also reduces unnecessary complexities in Android applications. Step 3: Create a new Wisdom app using the file.text sink. Note that we are using the namespace file.text to create this sink. WisdomApp wisdomApp = new WisdomApp(); wisdomApp.defineStream(\"StockStream\"); wisdomApp.defineStream(\"OutputStream\"); wisdomApp.defineQuery(\"query1\") .from(\"StockStream\") .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); wisdomApp.addSink(\"OutputStream\", Sink.create(\"file.text\", Map.of(\"path\", \"output.log\"))); wisdomApp.start(); InputHandler stockStreamInputHandler = wisdomApp.getInputHandler(\"StockStream\"); stockStreamInputHandler.send(EventGenerator.generate(\"symbol\", \"IBM\", \"price\", 50.0, \"volume\", 10)); stockStreamInputHandler.send(EventGenerator.generate(\"symbol\", \"WSO2\", \"price\", 60.0, \"volume\", 15));","title":"Use Extension in Java API"},{"location":"wisdom-extensions/#use-extension-in-wisdom-query","text":"Above sink can be used in a Wisdom query as given below: def stream StockStream; @sink(type='file.text', path='output.log') def stream OutputStream; from StockStream select symbol, price insert into OutputStream;JAR","title":"Use Extension in Wisdom Query"},{"location":"wisdom-extensions/#deploy-in-wisdom-server","text":"Step 1: Build the jar file containing com.javahelps.wisdom.extensions.file.sink.TextFileSink . mvn clean package Step 2: Copy and paste the target/xxx.jar file into WISDOM_HOME/lib directory. Step 3: Restart running Wisdom services.","title":"Deploy in Wisdom Server"},{"location":"query/aggregate/","text":"Aggregators aggregate events and inject results into the stream. Wisdom supports the following aggregators: SUM MIN MAX AVERAGE COUNT Java API: Find the total price of three stock events. app.defineQuery(\"query1\") .from(\"StockStream\") .window(Window.lengthBatch(3)) .aggregate(Operator.SUM(\"price\", \"total\")) .insertInto(\"OutputStream\"); Wisdom Query: Find the total price of three stock events. from StockStream window.lengthBatch(3) aggregate sum(price) as total insert into OutputStream;","title":"Aggregate"},{"location":"query/filter/","text":"Wisdom Filter filters events from a stream based on a given predicate. In Wisdom query, a filter can be used anywhere in between from and insert into or update statements. In Java API, the filter method accepts any java.util.function.Predicate<Event> as the argument. For user's convenient, Wisdom offers the following built-in predicates: Java API Query Operator Description Operator.EQUALS == Checks if left operand is equal to right operand Operator.GREATER_THAN > Checks if left operand is greater than right operand Operator.GREATER_THAN_OR_EQUAL >= Checks if left operand is greater than or equal to right operand Operator.LESS_THAN < Checks if left operand is less than right operand Operator.LESS_THAN_OR_EQUAL <= Checks if left operand is less than or equal to right operand Operator.IN in Checks if left operand is in right operand. Here the right operand can be a string or array Operator.MATCHES matches Checks if left regex matches in the right string Java API: Filter events having symbol equal to AMAZON . app.defineQuery(\"query1\") .from(\"StockStream\") .filter(event -> \"UWO\".equals(event.get(\"symbol\"))) .insertInto(\"OutputStream\"); Above code can be written using built-in Operator.EQUALS predicate as shown below: app.defineQuery(\"query1\") .from(\"StockStream\") .filter(Operator.EQUALS(Attribute.of(\"symbol\"), \"AMAZON\")) .insertInto(\"OutputStream\"); Wisdom Query: Filter events having symbol equal to AMAZON . from StockStream filter symbol == 'AMAZON' insert into OutputStream; Wisdom Query supports the following logical operators: == , > , >= , < , <= , in and matches","title":"Filter"},{"location":"query/map/","text":"Wisdom map is used to map events from one format to another format. In Java API, mapper accepts any java.util.function.Function<Event, Event> . Wisdom also provides the following built-in mappers: Wisdom Core library provides the built-in mappers. Mapper.RENAME Mapper.FORMAT_TIME Mapper.TO_INT Mapper.TO_LONG Mapper.TO_FLOAT Mapper.TO_DOUBLE Mapper.CONSTANT You can find the following mappers in the Wisdom Extension library: tensorFlow( <model-path> , operation , type ) - Used to call a Tensorflow model and insert the result into the stream. grpc( endpoint , select ) - Used to call a gRPC service and to insert the result into the stream http( endpoint , method select`) - Used to call an HTTP service and to insert the result into the stream Java API: Rename symbol to name and price to cost . app.defineQuery(\"query1\") .from(\"StockStream\") .map(Mapper.RENAME(\"symbol\", \"name\"), Mapper.RENAME(\"price\", \"cost\")) .select(\"name\", \"cost\") .insertInto(\"OutputStream\"); Wisdom Query: Rename symbol to name and price to cost . from StockStream map symbol as name, price as cost select name, cost insert into OutputStream; Conditional Mapping A map operator can be called only if a given condition is satisfied. A sample code to replace null symbol in a StockStream by UNKNOWN is given below. app.defineQuery(\"query1\") .from(\"StockStream\") .map(Mapper.CONSTANT(\"UNKNOWN\", \"symbol\").onlyIf(Operator.EQUALS(Attribute.of(\"symbol\"), null))) .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); Wisdom Query: Rename symbol to name and price to cost . from StockStream map 'UNKNOWN' as symbol if symbol == null select symbol, price insert into OutputStream;","title":"Map"},{"location":"query/map/#conditional-mapping","text":"A map operator can be called only if a given condition is satisfied. A sample code to replace null symbol in a StockStream by UNKNOWN is given below. app.defineQuery(\"query1\") .from(\"StockStream\") .map(Mapper.CONSTANT(\"UNKNOWN\", \"symbol\").onlyIf(Operator.EQUALS(Attribute.of(\"symbol\"), null))) .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); Wisdom Query: Rename symbol to name and price to cost . from StockStream map 'UNKNOWN' as symbol if symbol == null select symbol, price insert into OutputStream;","title":"Conditional Mapping"},{"location":"query/partition/","text":"Partitions split streams into partitions based on given attributes to sandbox aggregations and to parallelize execution. Wisdom supports two types of partitions. Partition by attribute Partition by value Partition by attribute is similar to other stream processors, generates partition keys by concatenating their attribute values. For example, the following query partitions packets based on their srcIp and dstIp . Packets sent from 127.0.0.1 to 127.0.0.2 are assigned to a different partition from the packets sent from 127.0.0.2 to 127.0.0.1 . Partition packets transferred from the same source to the same destination. from PacketStream partition by srcIp, dstIp select srcIp, dstIp, timestamp insert into OutputStream; Partition by value partitions events based on their actual values regardless of from which attributes they are from. For example, the following query partitions packets based on their srcIp and dstIp . Packets sent from 127.0.0.1 to 127.0.0.2 and packets sent from 127.0.0.2 to 127.0.0.1 are assigned to the same partition. Partition packets transferred between the same source and destination. from PacketStream partition by srcIp + dstIp select srcIp, dstIp, timestamp insert into OutputStream; Both partition by attribute and partition by value behaves in the way if the number of attributes used to partition is one. Java API: Partition StockStream by symbol app.defineQuery(\"query1\") .from(\"StockStream\") .partitionByAttr(\"symbol\") .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); app.defineQuery(\"query1\") .from(\"StockStream\") .partitionByVal(\"symbol\") .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); Wisdom Query: Partition StockStream by symbol from StockStream partition by symbol select symbol, price insert into OutputStream;","title":"Partition"},{"location":"query/pattern/","text":"Patterns are another important aspect of stream processing. Wisdom supports regular patterns, count patterns and logical patterns. Java API: Wisdom pattern to detect IBM followed by GOOGLE . app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); // e1 -> e2 Pattern e1 = Pattern.pattern(\"Pattern1\", \"e1\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"IBM\")); Pattern e2 = Pattern.pattern(\"Pattern2\", \"e2\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"GOOGLE\")); Pattern finalPattern = Pattern.followedBy(e1, e2); app.defineQuery(\"query1\") .from(finalPattern) .select(\"e1.symbol\", \"e2.symbol\") .insertInto(\"OutputStream\"); Wisdom pattern to detect IBM followed by GOOGLE within five minutes. app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); // e1 -> e2 within 300,000 milliseconds Pattern e1 = Pattern.pattern(\"Pattern1\", \"e1\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"IBM\")); Pattern e2 = Pattern.pattern(\"Pattern2\", \"e2\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"GOOGLE\")); Pattern finalPattern = Pattern.followedBy(e1, e2, 5*60*1000); app.defineQuery(\"query1\") .from(finalPattern) .select(\"e1.symbol\", \"e2.symbol\") .insertInto(\"OutputStream\"); Wisdom pattern to detect IBM or GOOGLE followed by ORACLE . app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); // e1 or e2 -> e3 Pattern e1 = Pattern.pattern(\"Pattern1\", \"e1\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"IBM\")); Pattern e2 = Pattern.pattern(\"Pattern2\", \"e2\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"GOOGLE\")); Pattern e3 = Pattern.pattern(\"Pattern3\", \"e3\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"ORACLE\")); Pattern finalPattern = Pattern.followedBy(Pattern.or(e1, e2), e3); app.defineQuery(\"query1\") .from(finalPattern) .select(\"e1.symbol\", \"e2.symbol\", \"e3.symbol\") .insertInto(\"OutputStream\"); Wisdom pattern to detect IBM and GOOGLE followed by ORACLE . app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); // e1 and e2 -> e3 Pattern e1 = Pattern.pattern(\"Pattern1\", \"e1\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"IBM\")); Pattern e2 = Pattern.pattern(\"Pattern2\", \"e2\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"GOOGLE\")); Pattern e3 = Pattern.pattern(\"Pattern3\", \"e3\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"ORACLE\")); Pattern finalPattern = Pattern.followedBy(Pattern.and(e1, e2), e3); app.defineQuery(\"query1\") .from(finalPattern) .select(\"e1.symbol\", \"e2.symbol\", \"e3.symbol\") .insertInto(\"OutputStream\"); Wisdom pattern to detect IBM but no GOOGLE before IBM . app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); // not e1 -> e2 Pattern e1 = Pattern.pattern(\"Pattern1\", \"e1\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"IBM\")); Pattern e2 = Pattern.pattern(\"Pattern2\", \"e2\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"GOOGLE\")); Pattern finalPattern = Pattern.followedBy(Pattern.not(e1), e2); app.defineQuery(\"query1\") .from(finalPattern) .select(\"e2.symbol\") .insertInto(\"OutputStream\"); Wisdom pattern to detect two to five number of IBM followed by GOOGLE . app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); // e1<2:5> -> e2 Pattern e1 = Pattern.pattern(\"Pattern1\", \"e1\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"IBM\")) .times(2, 5); Pattern e2 = Pattern.pattern(\"Pattern2\", \"e2\", \"StockStream\") .filter(event -> event.get(\"symbol\").equals(\"GOOGLE\")); Pattern finalPattern = Pattern.followedBy(e1, e2); app.defineQuery(\"query1\") .from(finalPattern) .select(\"e1[0].symbol\", \"e2.symbol\") .insertInto(\"OutputStream\"); Wisdom Query: Wisdom pattern to detect IBM followed by GOOGLE . def stream StockStream; def stream OutputStream; from StockStream[symbol == 'IBM'] as e1 -> StockStream[symbol == GOOGLE] as e2 select e1.symbol, e2.symbol insert into OutputStream; Wisdom pattern to detect IBM followed by GOOGLE within five minutes. def stream StockStream; def stream OutputStream; from StockStream[symbol == 'IBM'] as e1 -> StockStream[symbol == GOOGLE] as e2 within time.minutes(5) select e1.symbol, e2.symbol insert into OutputStream; Wisdom pattern to detect IBM or GOOGLE followed by ORACLE . def stream StockStream; def stream OutputStream; from (StockStream[symbol == 'IBM'] as e1 or StockStream[symbol == GOOGLE] as e2) -> StockStream[symbol == ORACLE] as e3 select e1.symbol, e2.symbol, e3.symbol insert into OutputStream; Wisdom pattern to detect IBM and GOOGLE followed by ORACLE . def stream StockStream; def stream OutputStream; from (StockStream[symbol == 'IBM'] as e1 and StockStream[symbol == GOOGLE] as e2) -> StockStream[symbol == ORACLE] as e3 select e1.symbol, e2.symbol, e3.symbol insert into OutputStream; Wisdom pattern to detect IBM but no GOOGLE before IBM . def stream StockStream; def stream OutputStream; from not StockStream[symbol == 'IBM'] -> StockStream[symbol == GOOGLE] as e2 select e2.symbol insert into OutputStream; Wisdom pattern to detect two to five number of IBM followed by GOOGLE . def stream StockStream; def stream OutputStream; from not StockStream[symbol == 'IBM']<2:5> as e1 -> StockStream[symbol == GOOGLE] as e2 select e1[0].symbol, e2.symbol insert into OutputStream;","title":"Pattern"},{"location":"query/query/","text":"A Wisdom application can be created either using Java API or Wisdom Query Language. Wisdom Query Language is a SQL like query inspired by Siddhi Query . A Wisdom query should follow this template: <app annotation>? ( <stream definition> | <variable definition> | ... ) + ( <query> ) + ; NOTE: Since I am busy with my research, I am unable to list all features implemented in Wisdom here. Once you get access to Wisdom, please check the Unit test classes to see the available features and how to use them. You can contact me at any time to clarify your issues.","title":"Query"},{"location":"query/select/","text":"Selector selects attributes and events to be inserted into the following operator. If the selector is used with attribute names, it selects the attributes from events. If a selector is used with the index of events after windows, it selects the specified events from the list of events. NOTE: Positive indices select events from the beginning of a list and negative indices select events from the end of a list. For example 0 selects the first event and -1 selects the last event. Java API: Select symbol and price from all events and insert them into OutputStream. app.defineQuery(\"query1\") .from(\"StockStream\") .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); Select last two events from window and insert them into OutputStream. app.defineQuery(\"query1\") .from(\"StockStream\") .window.lengthBatch(5) .select(Index.of(-2, -1)) .insertInto(\"OutputStream\"); Wisdom Query: Select symbol and price from all events and insert them into OutputStream. from StockStream select symbol, price insert into OutputStream; Select last two events from window and insert them into OutputStream. from StockStream window.lengthBatch(3) select -2, -1 insert into OutputStream;","title":"Select"},{"location":"query/sink/","text":"Sink receives events from a stream and send them to the relative receiver. Currently Wisdom supports the following sinks: HTTP Kafka Text File Console Java API: Define HTTP sink in Java. wisdomApp.defineQuery(\"query1\") .from(\"StockStream\") .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); wisdomApp.addSink(\"OutputStream\", new HTTPSink(\"http://localhost:9999/streamReceiver\")); Wisdom Query: Define HTTP sink in Wisdom Query. def stream StockStream; @sink(type='http', mapping='json', endpoint='http://localhost:9999/streamReceiver') def stream OutputStream; from StockStream select symbol, price insert into OutputStream;","title":"Sink"},{"location":"query/source/","text":"Source is an event source for a Wisdom stream. Currently Wisdom provides the following sources: HTTP Kafka CSV Pcap GRPC Java API: Define Kafka source in Java. wisdomApp.defineQuery(\"query1\") .from(\"StockStream\") .select(\"symbol\", \"price\") .insertInto(\"OutputStream\"); wisdomApp.addSource(\"StockStream\", new KafkaSource(\"localhost:9092\")); Wisdom Query: Define Kafka source in Wisdom Query. @source(type='kafka', bootstrap='localhost:9092') def stream StockStream; def stream OutputStream; from StockStream select symbol, price insert into OutputStream;","title":"Source"},{"location":"query/stream/","text":"A stream is the very basic component of stream processors. You can interpret a stream as the entry and exit points of a pipeline in a stream. Not like statically typed streams in other stream processors, Wisdom offers dynamically typed stream which can accept any attributes. However, an attribute of a stream can take only a java.lang.Comparable object. More details about supported data structures and how to pass lists will be covered later in another section. Stream Definition \u200b Similar to variables, a stream must be defined before using it in a query. A stream definition must provide a unique name. I recommend using PascalCase naming convention for stream names. Java API: WisdomApp app = new WisdomApp(); app.defineStream(\"StockStream\"); Wisdom Query: def stream StockStream; Stream in Query A Wisdom query must start with either a Stream or Pattern and ends with a Stream or Variable . In the following example, we fetch events from StockStream and feed them to OutputStream . Java API: WisdomApp app = new WisdomApp(); app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); Variable<Comparable> min_price = app.defineVariable(\"min_price\", 10L); app.defineQuery(\"query1\") .from(\"StockStream\") .insertInto(\"OutputStream\"); Wisdom Query: def stream StockStream; def stream OutputStream; from StockStream insert into OutputStream;","title":"Stream"},{"location":"query/stream/#stream-definition","text":"\u200b Similar to variables, a stream must be defined before using it in a query. A stream definition must provide a unique name. I recommend using PascalCase naming convention for stream names. Java API: WisdomApp app = new WisdomApp(); app.defineStream(\"StockStream\"); Wisdom Query: def stream StockStream;","title":"Stream Definition"},{"location":"query/stream/#stream-in-query","text":"A Wisdom query must start with either a Stream or Pattern and ends with a Stream or Variable . In the following example, we fetch events from StockStream and feed them to OutputStream . Java API: WisdomApp app = new WisdomApp(); app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); Variable<Comparable> min_price = app.defineVariable(\"min_price\", 10L); app.defineQuery(\"query1\") .from(\"StockStream\") .insertInto(\"OutputStream\"); Wisdom Query: def stream StockStream; def stream OutputStream; from StockStream insert into OutputStream;","title":"Stream in Query"},{"location":"query/variable/","text":"\u200b \u200b\u200bSimilar to programming languages, Wisdom variables are used to store values in the memory. A variable can store any java.lang.Comparable objects. Variable Definition \u200b Similar to streams, a variable must be defined before using it in a query. A variable definition must provide a unique name and a default value. Variable names should be lowercase, with words separated by underscores as necessary to improve readability. Java API: WisdomApp app = new WisdomApp(); app.defineVariable(\"min_price\", 10L); Wisdom Query: def variable min_price; Read Variable In Wisdom Java API, variables inherit java.util.function.Supplier . Therefore, anywhere you need a Supplier, you can use a Variable. In the following examples, a variable is used to define a dynamic filter. Java API: WisdomApp app = new WisdomApp(); app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); Variable<Comparable> minPrice = app.defineVariable(\"min_price\", 10L); app.defineQuery(\"query1\") .from(\"StockStream\") .filter(Operator.GREATER_THAN(\"price\", minPrice)) .insertInto(\"OutputStream\"); Wisdom Query: def stream StockStream; def stream OutputStream; def variable min_price = 10; from StockStream filter price > $min_price insert into OutputStream; Note the $ prefix to variable name in the above query to indicate the difference between attribute name and variable. Update Variable A variable can be updated externally using Java API or using an event in a query. To update a variable from an event, the event should have an attribute with the same name as the variable. Following queries, update the variable min_price with an assumption that events in VariableStream have an attribute value . Java API: WisdomApp app = new WisdomApp(); app.defineStream(\"VariableStream\"); app.defineVariable(\"min_price\", 3); app.defineQuery(\"query\") .from(\"VariableStream\") .map(Mapper.rename(\"value\", \"min_price\")) .update(\"min_price\"); Wisdom Query: def stream VariableStream; def variable min_price = 3; from VariableStream map value as min_price update min_price;","title":"Variable"},{"location":"query/variable/#variable-definition","text":"\u200b Similar to streams, a variable must be defined before using it in a query. A variable definition must provide a unique name and a default value. Variable names should be lowercase, with words separated by underscores as necessary to improve readability. Java API: WisdomApp app = new WisdomApp(); app.defineVariable(\"min_price\", 10L); Wisdom Query: def variable min_price;","title":"Variable Definition"},{"location":"query/variable/#read-variable","text":"In Wisdom Java API, variables inherit java.util.function.Supplier . Therefore, anywhere you need a Supplier, you can use a Variable. In the following examples, a variable is used to define a dynamic filter. Java API: WisdomApp app = new WisdomApp(); app.defineStream(\"StockStream\"); app.defineStream(\"OutputStream\"); Variable<Comparable> minPrice = app.defineVariable(\"min_price\", 10L); app.defineQuery(\"query1\") .from(\"StockStream\") .filter(Operator.GREATER_THAN(\"price\", minPrice)) .insertInto(\"OutputStream\"); Wisdom Query: def stream StockStream; def stream OutputStream; def variable min_price = 10; from StockStream filter price > $min_price insert into OutputStream; Note the $ prefix to variable name in the above query to indicate the difference between attribute name and variable.","title":"Read Variable"},{"location":"query/variable/#update-variable","text":"A variable can be updated externally using Java API or using an event in a query. To update a variable from an event, the event should have an attribute with the same name as the variable. Following queries, update the variable min_price with an assumption that events in VariableStream have an attribute value . Java API: WisdomApp app = new WisdomApp(); app.defineStream(\"VariableStream\"); app.defineVariable(\"min_price\", 3); app.defineQuery(\"query\") .from(\"VariableStream\") .map(Mapper.rename(\"value\", \"min_price\")) .update(\"min_price\"); Wisdom Query: def stream VariableStream; def variable min_price = 3; from VariableStream map value as min_price update min_price;","title":"Update Variable"},{"location":"query/window/","text":"Windows are used to batch events based on some conditions. Wisdom 0.0.1 supports the following windows: Window.length Window.lengthBatch Window.externalTimeBatch UniqueWindow.lengthBatch UniqueWindow.externalTimeBatch Java API: app.defineQuery(\"query1\") .from(\"StockStream\") .window(Window.lengthBatch(3)) .insertInto(\"OutputStream\"); Wisdom Query: from StockStream window.lengthBatch(3) insert into OutputStream;","title":"Window"},{"location":"use-cases/auto-scaling/","text":"Functionally Auto-scaling Deployment Following queries were developed to test functionally auto-scaling deployment of Wisdom for Test 2 in Real-time Intrusion Detection in Network Traffic Using Adaptive and Self-boosting Stream Processor . The following diagram depicts the deployment setup: Packet Filter @app(name='packet_filter', version='1.0.0', priority=10, stats='StatisticsStream', stats_freq=time.sec(5), stats_vars=['port']) @source(type='kafka', bootstrap='localhost:9092', topic='PacketStream') def stream PacketStream; @config(stats=true) @sink(type='kafka', bootstrap='localhost:9092', topic='PossibleDosStream') def stream PossibleDosStream; @config(stats=true) @sink(type='kafka', bootstrap='localhost:9092', topic='PossibleBruteForceStream') def stream PossibleBruteForceStream; @config(stats=true) @sink(type='kafka', bootstrap='localhost:9092', topic='PossiblePortScanStream') def stream PossiblePortScanStream; @sink(type='kafka', bootstrap='localhost:9092', topic='_Statistics') def stream StatisticsStream; @query(name='FilterDosAttacks') from PacketStream filter 'http' == app_protocol and destPort == 80 and '\\r\\n\\r\\n' in data and 'Keep-Alive: \\\\d+' in data insert into PossibleDosStream; @query(name='FilterBruteForceAttacks') from PacketStream filter 'FTP[Control]' == app_protocol and '530 Login incorrect' in data insert into PossibleBruteForceStream; @query(name='FilterPortScanAttacks') from PacketStream filter syn == true and ack == false insert into PossiblePortScanStream; FTP Brute Force Attack @app(name='ftp_detector', version='1.0.0', priority=5, requires=['PossibleBruteForceStream']) @source(type='http', mapping='json') @source(type='kafka', bootstrap='localhost:9092', topic='PossibleBruteForceStream') def stream PossibleBruteForceStream; @sink(type='file.text', path='/home/gobinath/ftp.txt') def stream FTPAttackStream; from PossibleBruteForceStream partition by destIp window.externalTimeBatch('timestamp', 2940) aggregate count() as no_of_packets filter no_of_packets >= 3 select srcIp, destIp, no_of_packets, timestamp insert into FTPAttackStream; HTTP Slow Header Attack @app(name='dos_detector', version='1.0.0', priority=5, requires=['PossibleDosStream']) @source(type='http', mapping='json') @source(type='kafka', bootstrap='localhost:9092', topic='PossibleDosStream') def stream PossibleDosStream; @sink(type='file.text', path='/home/gobinath/dos.txt') def stream DosAttackStream; from PossibleDosStream partition by destIp window.externalTimeBatch('timestamp', 1189) aggregate count() as no_of_packets filter no_of_packets >= 3 select srcIp, destIp, no_of_packets, timestamp insert into DosAttackStream; Port Scanning @app(name='port_scan', version='1.0.0', priority=5, requires=['PossiblePortScanStream']) @source(type='http', mapping='json') @source(type='kafka', bootstrap='localhost:9092', topic='PossiblePortScanStream') def stream PossiblePortScanStream; @sink(type='file.text', path='/home/gobinath/port.txt') def stream PortScanStream; from PossiblePortScanStream partition by srcIp, destIp window.unique:externalTimeBatch('destPort', 'timestamp', 108) aggregate count() as no_of_packets filter no_of_packets >= 9 select srcIp, destIp, no_of_packets, timestamp insert into PortScanStream;","title":"Auto-scaling Deployment"},{"location":"use-cases/auto-scaling/#functionally-auto-scaling-deployment","text":"Following queries were developed to test functionally auto-scaling deployment of Wisdom for Test 2 in Real-time Intrusion Detection in Network Traffic Using Adaptive and Self-boosting Stream Processor . The following diagram depicts the deployment setup:","title":"Functionally Auto-scaling Deployment"},{"location":"use-cases/auto-scaling/#packet-filter","text":"@app(name='packet_filter', version='1.0.0', priority=10, stats='StatisticsStream', stats_freq=time.sec(5), stats_vars=['port']) @source(type='kafka', bootstrap='localhost:9092', topic='PacketStream') def stream PacketStream; @config(stats=true) @sink(type='kafka', bootstrap='localhost:9092', topic='PossibleDosStream') def stream PossibleDosStream; @config(stats=true) @sink(type='kafka', bootstrap='localhost:9092', topic='PossibleBruteForceStream') def stream PossibleBruteForceStream; @config(stats=true) @sink(type='kafka', bootstrap='localhost:9092', topic='PossiblePortScanStream') def stream PossiblePortScanStream; @sink(type='kafka', bootstrap='localhost:9092', topic='_Statistics') def stream StatisticsStream; @query(name='FilterDosAttacks') from PacketStream filter 'http' == app_protocol and destPort == 80 and '\\r\\n\\r\\n' in data and 'Keep-Alive: \\\\d+' in data insert into PossibleDosStream; @query(name='FilterBruteForceAttacks') from PacketStream filter 'FTP[Control]' == app_protocol and '530 Login incorrect' in data insert into PossibleBruteForceStream; @query(name='FilterPortScanAttacks') from PacketStream filter syn == true and ack == false insert into PossiblePortScanStream;","title":"Packet Filter"},{"location":"use-cases/auto-scaling/#ftp-brute-force-attack","text":"@app(name='ftp_detector', version='1.0.0', priority=5, requires=['PossibleBruteForceStream']) @source(type='http', mapping='json') @source(type='kafka', bootstrap='localhost:9092', topic='PossibleBruteForceStream') def stream PossibleBruteForceStream; @sink(type='file.text', path='/home/gobinath/ftp.txt') def stream FTPAttackStream; from PossibleBruteForceStream partition by destIp window.externalTimeBatch('timestamp', 2940) aggregate count() as no_of_packets filter no_of_packets >= 3 select srcIp, destIp, no_of_packets, timestamp insert into FTPAttackStream;","title":"FTP Brute Force Attack"},{"location":"use-cases/auto-scaling/#http-slow-header-attack","text":"@app(name='dos_detector', version='1.0.0', priority=5, requires=['PossibleDosStream']) @source(type='http', mapping='json') @source(type='kafka', bootstrap='localhost:9092', topic='PossibleDosStream') def stream PossibleDosStream; @sink(type='file.text', path='/home/gobinath/dos.txt') def stream DosAttackStream; from PossibleDosStream partition by destIp window.externalTimeBatch('timestamp', 1189) aggregate count() as no_of_packets filter no_of_packets >= 3 select srcIp, destIp, no_of_packets, timestamp insert into DosAttackStream;","title":"HTTP Slow Header Attack"},{"location":"use-cases/auto-scaling/#port-scanning","text":"@app(name='port_scan', version='1.0.0', priority=5, requires=['PossiblePortScanStream']) @source(type='http', mapping='json') @source(type='kafka', bootstrap='localhost:9092', topic='PossiblePortScanStream') def stream PossiblePortScanStream; @sink(type='file.text', path='/home/gobinath/port.txt') def stream PortScanStream; from PossiblePortScanStream partition by srcIp, destIp window.unique:externalTimeBatch('destPort', 'timestamp', 108) aggregate count() as no_of_packets filter no_of_packets >= 9 select srcIp, destIp, no_of_packets, timestamp insert into PortScanStream;","title":"Port Scanning"},{"location":"use-cases/intrusion-detection/","text":"Intrusion Detection Following queries were developed to detect network attacks in CICIDS 2017 dataset. We developed the following rules based on facts behind each attacks. Even though we obtained an average precision of 99.98%, we are not responsible for failure of detecting real time attacks. FTP Brute Force Attack In FTP Brute Force attack, an attacker tries different combinations of username and password to login to the FTP server. Therefore, there should be significantly large amount of failed attempts within a short period of time. @app(name='FTPBruteForceDetector', version='1.0.0') def stream PacketStream; def stream AttackStream; def variable time_threshold = time.sec(1); def variable count_threshold = 7; from PacketStream filter 'FTP' == app_protocol and '530 Login incorrect' in data partition by destIp window.externalTimeBatch('timestamp', $time_threshold) aggregate count() as no_of_packets filter no_of_packets >= $count_threshold select srcIp, destIp, no_of_packets, timestamp insert into AttackStream; HTTP Slow Header Attack HTTP Slow Header attack is a Denial of Service(DOS) attack in which a victim server is compromized by sending too many HTTP incomplete requests with random Keep-Alive time. For more details, read: How Secure are Web Servers? An Empirical Study of Slow HTTP DoS Attacks and Detection . @app(name='SlowHeaderDetector', version='1.0.0') def stream PacketStream; def stream AttackStream; def variable time_threshold = time.sec(1); def variable count_threshold = 998; from PacketStream filter 'http' == app_protocol and destPort == 80 and '\\r\\n\\r\\n' in data and 'Keep-Alive: \\\\d+' in data partition by destIp window.externalTimeBatch('timestamp', $time_threshold) aggregate count() as no_of_packets filter no_of_packets >= $count_threshold select srcIp, destIp, no_of_packets, timestamp insert into AttackStream; Port Scanning Even though Port Scanning is a common technique used by attackers, it is hard to fit all types of port scans into a single CEP rule. The following rule is developed to detect nmap -sS port scan. For more details, please visit Port Scanning Techniques . @app(name='PortScanDetector', version='1.0.0') def stream PacketStream; def stream AttackStream; @config(trainable=true, minimum=100, maximum=60000, step=-1) def variable time_threshold = 761; @config(trainable=true, minimum=3, maximum=1000, step=1) def variable count_threshold = 3; from PacketStream filter syn == true and ack == false partition by srcIp, destIp window.unique:externalTimeBatch('destPort', 'timestamp', $time_threshold) aggregate count() as no_of_packets filter no_of_packets >= $count_threshold select srcIp, destIp, no_of_packets, timestamp insert into AttackStream;","title":"Intrusion Detection"},{"location":"use-cases/intrusion-detection/#intrusion-detection","text":"Following queries were developed to detect network attacks in CICIDS 2017 dataset. We developed the following rules based on facts behind each attacks. Even though we obtained an average precision of 99.98%, we are not responsible for failure of detecting real time attacks.","title":"Intrusion Detection"},{"location":"use-cases/intrusion-detection/#ftp-brute-force-attack","text":"In FTP Brute Force attack, an attacker tries different combinations of username and password to login to the FTP server. Therefore, there should be significantly large amount of failed attempts within a short period of time. @app(name='FTPBruteForceDetector', version='1.0.0') def stream PacketStream; def stream AttackStream; def variable time_threshold = time.sec(1); def variable count_threshold = 7; from PacketStream filter 'FTP' == app_protocol and '530 Login incorrect' in data partition by destIp window.externalTimeBatch('timestamp', $time_threshold) aggregate count() as no_of_packets filter no_of_packets >= $count_threshold select srcIp, destIp, no_of_packets, timestamp insert into AttackStream;","title":"FTP Brute Force Attack"},{"location":"use-cases/intrusion-detection/#http-slow-header-attack","text":"HTTP Slow Header attack is a Denial of Service(DOS) attack in which a victim server is compromized by sending too many HTTP incomplete requests with random Keep-Alive time. For more details, read: How Secure are Web Servers? An Empirical Study of Slow HTTP DoS Attacks and Detection . @app(name='SlowHeaderDetector', version='1.0.0') def stream PacketStream; def stream AttackStream; def variable time_threshold = time.sec(1); def variable count_threshold = 998; from PacketStream filter 'http' == app_protocol and destPort == 80 and '\\r\\n\\r\\n' in data and 'Keep-Alive: \\\\d+' in data partition by destIp window.externalTimeBatch('timestamp', $time_threshold) aggregate count() as no_of_packets filter no_of_packets >= $count_threshold select srcIp, destIp, no_of_packets, timestamp insert into AttackStream;","title":"HTTP Slow Header Attack"},{"location":"use-cases/intrusion-detection/#port-scanning","text":"Even though Port Scanning is a common technique used by attackers, it is hard to fit all types of port scans into a single CEP rule. The following rule is developed to detect nmap -sS port scan. For more details, please visit Port Scanning Techniques . @app(name='PortScanDetector', version='1.0.0') def stream PacketStream; def stream AttackStream; @config(trainable=true, minimum=100, maximum=60000, step=-1) def variable time_threshold = 761; @config(trainable=true, minimum=3, maximum=1000, step=1) def variable count_threshold = 3; from PacketStream filter syn == true and ack == false partition by srcIp, destIp window.unique:externalTimeBatch('destPort', 'timestamp', $time_threshold) aggregate count() as no_of_packets filter no_of_packets >= $count_threshold select srcIp, destIp, no_of_packets, timestamp insert into AttackStream;","title":"Port Scanning"},{"location":"use-cases/tensor-flow/","text":"TensorFlow Integration Stream Processors often require to support machine learning. Wisdom comes with built-in support to serve TensorFlow models. To serve a TensorFlow model, build a model first in Python (or whatever the way you prefer) and export it to a file. Then, Wisdom can serve that model within the Wisdom environment. Build Model Execute the following Python 3 script to export a TensorFlow model to /tmp/tf_add_model which receives two integers x and y and produce ans which is the sum of x and y . #!/usr/bin/env python3 import tensorflow as tf from tensorflow.python.saved_model import builder as saved_model_builder from tensorflow.python.saved_model import signature_constants from tensorflow.python.saved_model import signature_def_utils from tensorflow.python.saved_model import tag_constants from tensorflow.python.saved_model.utils import build_tensor_info x = tf.placeholder(tf.int32, name='x') y = tf.placeholder(tf.int32, name='y') # This is our model add = tf.add(x, y, name='ans') with tf.Session() as sess: # Pick out the model input and output x_tensor = sess.graph.get_tensor_by_name('x:0') y_tensor = sess.graph.get_tensor_by_name('y:0') ans_tensor = sess.graph.get_tensor_by_name('ans:0') x_info = build_tensor_info(x_tensor) y_info = build_tensor_info(y_tensor) ans_info = build_tensor_info(ans_tensor) # Create a signature definition for tfserving signature_definition = signature_def_utils.build_signature_def( inputs={'x': x_info, 'y': y_info}, outputs={'ans': ans_info}, method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME) builder = saved_model_builder.SavedModelBuilder('/tmp/tf_add_model') builder.add_meta_graph_and_variables( sess, [tag_constants.SERVING], signature_def_map={ signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_definition }) # Save the model so we can serve it with a model server :) builder.save() Wisdom Query def stream EventStream; def stream OutputStream; from EventStream map int('x') as x, int('y') as y map tensorFlow(path='/tmp/tf_add_model', operation='ans', type='int') as result insert into OutputStream; Above query feeds x and y to the TensorFlow model and inject the output ans as result in the output event. In this query, path is the location of the model, operation is the output Tensor name, and type is the data type of the output Tensor. The type must be any of the following: int , long , float , double , bool . Sending an event {x: 10, y: 20} to EventStream will emit {x: 10, y: 20, result: 30} in the OutputStream.","title":"TensorFlow Integration"},{"location":"use-cases/tensor-flow/#tensorflow-integration","text":"Stream Processors often require to support machine learning. Wisdom comes with built-in support to serve TensorFlow models. To serve a TensorFlow model, build a model first in Python (or whatever the way you prefer) and export it to a file. Then, Wisdom can serve that model within the Wisdom environment.","title":"TensorFlow Integration"},{"location":"use-cases/tensor-flow/#build-model","text":"Execute the following Python 3 script to export a TensorFlow model to /tmp/tf_add_model which receives two integers x and y and produce ans which is the sum of x and y . #!/usr/bin/env python3 import tensorflow as tf from tensorflow.python.saved_model import builder as saved_model_builder from tensorflow.python.saved_model import signature_constants from tensorflow.python.saved_model import signature_def_utils from tensorflow.python.saved_model import tag_constants from tensorflow.python.saved_model.utils import build_tensor_info x = tf.placeholder(tf.int32, name='x') y = tf.placeholder(tf.int32, name='y') # This is our model add = tf.add(x, y, name='ans') with tf.Session() as sess: # Pick out the model input and output x_tensor = sess.graph.get_tensor_by_name('x:0') y_tensor = sess.graph.get_tensor_by_name('y:0') ans_tensor = sess.graph.get_tensor_by_name('ans:0') x_info = build_tensor_info(x_tensor) y_info = build_tensor_info(y_tensor) ans_info = build_tensor_info(ans_tensor) # Create a signature definition for tfserving signature_definition = signature_def_utils.build_signature_def( inputs={'x': x_info, 'y': y_info}, outputs={'ans': ans_info}, method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME) builder = saved_model_builder.SavedModelBuilder('/tmp/tf_add_model') builder.add_meta_graph_and_variables( sess, [tag_constants.SERVING], signature_def_map={ signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_definition }) # Save the model so we can serve it with a model server :) builder.save()","title":"Build Model"},{"location":"use-cases/tensor-flow/#wisdom-query","text":"def stream EventStream; def stream OutputStream; from EventStream map int('x') as x, int('y') as y map tensorFlow(path='/tmp/tf_add_model', operation='ans', type='int') as result insert into OutputStream; Above query feeds x and y to the TensorFlow model and inject the output ans as result in the output event. In this query, path is the location of the model, operation is the output Tensor name, and type is the data type of the output Tensor. The type must be any of the following: int , long , float , double , bool . Sending an event {x: 10, y: 20} to EventStream will emit {x: 10, y: 20, result: 30} in the OutputStream.","title":"Wisdom Query"}]}